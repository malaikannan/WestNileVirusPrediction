{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble, preprocessing\n",
    "\n",
    "#setting rootdirectory path \n",
    "rootdir = '/usr/bin/MachineLearning/WestNileVirusPrediction/'\n",
    "\n",
    "# Load Train Dataframe, Test DataFrame and Weather DataFrame \n",
    "\n",
    "train = pd.read_csv(rootdir + 'train.csv')\n",
    "test = pd.read_csv(rootdir + 'test.csv')\n",
    "weather = pd.read_csv(rootdir + 'weather.csv')\n",
    "sample = pd.read_csv(rootdir+'sampleSubmission.csv')\n",
    "\n",
    "\n",
    "# Get Target Value into Labels Array\n",
    "labels = train.WnvPresent.values\n",
    "\n",
    "# Not using codesum for this run\n",
    "weather = weather.drop('CodeSum', axis=1)\n",
    "\n",
    "# Split station 1 and 2 and join horizontally\n",
    "weather_stn1 = weather[weather['Station']==1]\n",
    "weather_stn2 = weather[weather['Station']==2]\n",
    "weather_stn1 = weather_stn1.drop('Station', axis=1)\n",
    "weather_stn2 = weather_stn2.drop('Station', axis=1)\n",
    "weather = weather_stn1.merge(weather_stn2, on='Date')\n",
    "\n",
    "# replace some missing values and T with -1\n",
    "weather = weather.replace('M', -1)\n",
    "weather = weather.replace('-', -1)\n",
    "weather = weather.replace('T', -1)\n",
    "weather = weather.replace(' T', -1)\n",
    "weather = weather.replace('  T', -1)\n",
    "\n",
    "# Functions to extract month and day from dataset\n",
    "# You can also use parse_dates of Pandas.\n",
    "def create_month(x):\n",
    "    return x.split('-')[1]\n",
    "\n",
    "def create_day(x):\n",
    "    return x.split('-')[2]\n",
    "\n",
    "train['month'] = train.Date.apply(create_month)\n",
    "train['day'] = train.Date.apply(create_day)\n",
    "test['month'] = test.Date.apply(create_month)\n",
    "test['day'] = test.Date.apply(create_day)\n",
    "\n",
    "# Add integer latitude/longitude columns\n",
    "train['Lat_int'] = train.Latitude.apply(int)\n",
    "train['Long_int'] = train.Longitude.apply(int)\n",
    "test['Lat_int'] = test.Latitude.apply(int)\n",
    "test['Long_int'] = test.Longitude.apply(int)\n",
    "\n",
    "# drop address columns\n",
    "train = train.drop(['Address', 'AddressNumberAndStreet','WnvPresent', 'NumMosquitos'], axis = 1)\n",
    "test = test.drop(['Id', 'Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "\n",
    "# Merge with weather data\n",
    "train = train.merge(weather, on='Date')\n",
    "test = test.merge(weather, on='Date')\n",
    "train = train.drop(['Date'], axis = 1)\n",
    "test = test.drop(['Date'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_dataframe.ix[test_dataframe[\"Species\"]=='UNSPECIFIED CULEX'].Species = 'CULEX ERRATICUS'\n",
    "test.loc[test.Species=='UNSPECIFIED CULEX','Species'] = 'CULEX ERRATICUS'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dummy variables for Species in Train and Test DataFrame\n",
    "train_Species_dummies = pd.get_dummies(train['Species'], prefix='Species')\n",
    "train_Species_dummies = train_Species_dummies.drop('Species_CULEX ERRATICUS',axis = 1)\n",
    "train = train.drop('Species',1)\n",
    "train = train.join(train_Species_dummies.ix[:,:])\n",
    "\n",
    "test_Species_dummies = pd.get_dummies(test['Species'], prefix='Species')\n",
    "test_Species_dummies = test_Species_dummies.drop('Species_CULEX ERRATICUS',axis = 1)\n",
    "test= test.drop('Species',1)\n",
    "test = test.join(test_Species_dummies.ix[:,:])\n",
    "\n",
    "#Creating dummy variables for Block in Train and Test DataFrame\n",
    "train_Block_dummies = pd.get_dummies(train['Block'], prefix='Block')\n",
    "train_Block_dummies = train_Block_dummies.drop('Block_10',axis = 1)\n",
    "train = train.drop('Block',1)\n",
    "train = train.join(train_Block_dummies.ix[:,:])\n",
    "\n",
    "test_Block_dummies = pd.get_dummies(test['Block'], prefix='Block')\n",
    "test_Block_dummies = test_Block_dummies.drop('Block_10',axis = 1)\n",
    "test= test.drop('Block',1)\n",
    "test = test.join(test_Block_dummies.ix[:,:])\n",
    "\n",
    "\n",
    "#Creating dummy variables for Street in Train and Test DataFrame\n",
    "train_Street_dummies = pd.get_dummies(train['Street'], prefix='Street')\n",
    "train_Street_dummies = train_Street_dummies.drop(train_Street_dummies.columns[0],axis = 1)\n",
    "train = train.drop('Street',1)\n",
    "train = train.join(train_Street_dummies.ix[:,:])\n",
    "\n",
    "test_Street_dummies = pd.get_dummies(test['Street'], prefix='Street')\n",
    "test_Street_dummies = test_Street_dummies.drop(test_Street_dummies.columns[0],axis = 1)\n",
    "test = test.drop('Street',1)\n",
    "test = test.join(test_Street_dummies.ix[:,:])\n",
    "\n",
    "#Creating dummy variables for Trap in Train and Test DataFrame\n",
    "train_Trap_dummies = pd.get_dummies(train['Trap'], prefix='Trap')\n",
    "train_Trap_dummies = train_Trap_dummies.drop(train_Trap_dummies.columns[0],axis = 1)\n",
    "train = train.drop('Trap',1)\n",
    "train = train.join(train_Trap_dummies.ix[:,:])\n",
    "\n",
    "test_Trap_dummies = pd.get_dummies(test['Trap'], prefix='Trap')\n",
    "test_Trap_dummies = test_Trap_dummies.drop(test_Trap_dummies.columns[0],axis = 1)\n",
    "test = test.drop('Trap',1)\n",
    "test = test.join(test_Trap_dummies.ix[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding out Missing columns in Train and Test Dataframe after Dummy Variable creation. \n",
    "# Without this step there will be a difference in columns between Train and Test DataFrame.\n",
    "# Machine Algorithm will throw an error without this step\n",
    "\n",
    "train_column_names_set = set(list(train.columns.values))\n",
    "test_column_names_set = set(list(test.columns.values))\n",
    "\n",
    "train_columns_notin_test = train_column_names_set - test_column_names_set\n",
    "test_columns_notin_train = test_column_names_set - train_column_names_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_columns_notin_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_columns_notin_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping columns in Test that are not in Train Data Frame\n",
    "test = test.drop(['Block_26', 'Street_ E 136TH ST', 'Street_ N KENNETH AVE', 'Street_ N KILBOURN AVE', 'Street_ N MELVINA AVE', 'Street_ S BALTIMORE AVE', 'Street_ S LOOMIS ST', 'Street_ S OGLESBY AVE', 'Street_ W 112TH ST', 'Street_ W 120TH ST', 'Street_ W 63RD PL', 'Street_ W DAKIN ST', 'Trap_T002A', 'Trap_T002B', 'Trap_T065A', 'Trap_T090A', 'Trap_T090B','Trap_T090C', 'Trap_T128A', 'Trap_T200A', 'Trap_T200B', 'Trap_T218A', 'Trap_T218B', 'Trap_T218C', 'Trap_T234'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns with -1s\n",
    "train = train.ix[:,(train != -1).any(axis=0)]\n",
    "test = test.ix[:,(test != -1).any(axis=0)]\n",
    "\n",
    "# Random Forest Classifier \n",
    "clf = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=1000)\n",
    "clf.fit(train, labels)\n",
    "\n",
    "# create predictions and submission file\n",
    "# currently this is one giving best results with the score of 0.71560\n",
    "#\n",
    "predictions = clf.predict_proba(test)[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv(rootdir+'beat_the_benchmark.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = train.values\n",
    "test_array = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=45, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.797668627018\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier implementation for prediction\n",
    "#Splitting Training result set into 2 for cross validation\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "tuned_parameters = {'n_estimators': [25, 45, 60], 'max_depth': [None, 1, 2, 3], 'min_samples_split': [1, 2, 3] ,'max_features' : ['auto','sqrt','log2']}\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_array,labels, test_size=0.20, random_state=0)\n",
    "\n",
    "clf = GridSearchCV(ensemble.RandomForestClassifier(n_jobs=-1), tuned_parameters, cv=10, scoring='roc_auc')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create predictions and submission file\n",
    "predictions = clf.predict_proba(test)[:,1]\n",
    "sample['WnvPresent'] = predictions\n",
    "sample.to_csv(rootdir+'beat_the_benchmark.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
